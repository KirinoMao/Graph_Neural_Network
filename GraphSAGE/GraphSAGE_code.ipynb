{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义聚类以及编码方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple supervised GraphSAGE model as well as examples running the model\n",
    "on the Cora and Pubmed datasets.\n",
    "\"\"\"\n",
    "\n",
    "class MeanAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregates a node's embeddings using mean of neighbors' embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, features, cuda=False, gcn=False): \n",
    "        \"\"\"\n",
    "        Initializes the aggregator for a specific graph.\n",
    "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
    "        cuda -- whether to use GPU\n",
    "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
    "        \"\"\"\n",
    "\n",
    "        super(MeanAggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "        \n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        \"\"\"\n",
    "        nodes --- list of nodes in a batch\n",
    "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
    "        num_sample --- number of neighbors to sample. No sampling if None.\n",
    "        \"\"\"\n",
    "        # Local pointers to functions (speed hack)\n",
    "        _set = set\n",
    "        \n",
    "        # 对于节点的邻居进行采样，允许每个节点的最多邻居个数为num_sample;\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, \n",
    "                            num_sample,\n",
    "                            )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "\n",
    "        if self.gcn:\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        \n",
    "        # 搜集采样后出现的节点情况\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
    "        \n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "        \n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n",
    "        \n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        \n",
    "        mask[row_indices, column_indices] = 1\n",
    "        # 说明在图结构中哪些节点之间存在边，稀疏存储方式；\n",
    "        # 注意mask格式为variable\n",
    "        \n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        mask = mask.div(num_neigh)\n",
    "        # 归一化操作\n",
    "        \n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        return to_feats\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a node's using 'convolutional' GraphSage approach\n",
    "    \"\"\"\n",
    "    def __init__(self, features, feature_dim, \n",
    "            embed_dim, adj_lists, aggregator,\n",
    "            num_sample=10,\n",
    "            base_model=None, gcn=False, cuda=False, \n",
    "            feature_transform=False): \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        self.gcn = gcn\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(\n",
    "                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes     -- list of nodes\n",
    "        \"\"\"\n",
    "        neigh_feats = self.aggregator.forward(nodes,\n",
    "                    [self.adj_lists[int(node)] for node in nodes], self.num_sample)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        else:\n",
    "            combined = neigh_feats\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        return combined\n",
    "\n",
    "\n",
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        scores = self.weight.mm(embeds)\n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加载数据并最终运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python_extra\\lib\\site-packages\\ipykernel_launcher.py:98: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "D:\\python_extra\\lib\\site-packages\\ipykernel_launcher.py:127: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.9350121021270752\n",
      "1 1.9036171436309814\n",
      "2 1.8774662017822266\n",
      "3 1.8476643562316895\n",
      "4 1.8220505714416504\n",
      "5 1.7541861534118652\n",
      "6 1.7160216569900513\n",
      "7 1.666232943534851\n",
      "8 1.5994302034378052\n",
      "9 1.5021945238113403\n",
      "10 1.517027735710144\n",
      "11 1.424843192100525\n",
      "12 1.3629202842712402\n",
      "13 1.3269981145858765\n",
      "14 1.2992253303527832\n",
      "15 1.2120598554611206\n",
      "16 1.1073676347732544\n",
      "17 1.1353802680969238\n",
      "18 1.0184845924377441\n",
      "19 1.0724217891693115\n",
      "20 0.881645143032074\n",
      "21 0.8489241003990173\n",
      "22 0.8724240064620972\n",
      "23 0.7524313926696777\n",
      "24 0.765596330165863\n",
      "25 0.6762802600860596\n",
      "26 0.668910562992096\n",
      "27 0.6819023489952087\n",
      "28 0.6338803768157959\n",
      "29 0.6307815909385681\n",
      "30 0.5287964344024658\n",
      "31 0.6115193367004395\n",
      "32 0.6080549359321594\n",
      "33 0.5754252076148987\n",
      "34 0.6476370692253113\n",
      "35 0.647040069103241\n",
      "36 1.0805492401123047\n",
      "37 0.6382949948310852\n",
      "38 0.5936675667762756\n",
      "39 0.4220713973045349\n",
      "40 0.42363980412483215\n",
      "41 0.4288618564605713\n",
      "42 0.389309823513031\n",
      "43 0.4071040749549866\n",
      "44 0.3543344736099243\n",
      "45 0.41788023710250854\n",
      "46 0.3668692708015442\n",
      "47 0.3726499676704407\n",
      "48 0.34820571541786194\n",
      "49 0.32611405849456787\n",
      "50 0.3413448631763458\n",
      "51 0.3155740797519684\n",
      "52 0.3373813033103943\n",
      "53 0.34284713864326477\n",
      "54 0.29745668172836304\n",
      "55 0.2569936513900757\n",
      "56 0.3432062268257141\n",
      "57 0.32080939412117004\n",
      "58 0.31968098878860474\n",
      "59 0.24962665140628815\n",
      "60 0.3106956481933594\n",
      "61 0.24543841183185577\n",
      "62 0.3228955864906311\n",
      "63 0.34345656633377075\n",
      "64 0.45810404419898987\n",
      "65 0.5089137554168701\n",
      "66 0.44487956166267395\n",
      "67 0.24900685250759125\n",
      "68 0.2553446888923645\n",
      "69 0.27904096245765686\n",
      "70 0.2168966829776764\n",
      "71 0.2546858787536621\n",
      "72 0.32099223136901855\n",
      "73 0.24916622042655945\n",
      "74 0.29961735010147095\n",
      "75 0.2176884114742279\n",
      "76 0.24045541882514954\n",
      "77 0.2624002993106842\n",
      "78 0.19373787939548492\n",
      "79 0.2275816798210144\n",
      "80 0.16820208728313446\n",
      "81 0.1638588309288025\n",
      "82 0.1981908679008484\n",
      "83 0.16195304691791534\n",
      "84 0.19445767998695374\n",
      "85 0.13335107266902924\n",
      "86 0.16056150197982788\n",
      "87 0.1528477966785431\n",
      "88 0.22950860857963562\n",
      "89 0.21997271478176117\n",
      "90 0.1668626368045807\n",
      "91 0.19354234635829926\n",
      "92 0.18276947736740112\n",
      "93 0.12523473799228668\n",
      "94 0.16518980264663696\n",
      "95 0.14163516461849213\n",
      "96 0.14942722022533417\n",
      "97 0.24483346939086914\n"
     ]
    }
   ],
   "source": [
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes,1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "    with open(\"./cora/cora.content\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            feat_data[i,:] = [float(x) for x in info[1:-1]]\n",
    "            node_map[info[0]] = i\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]\n",
    "\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"./cora/cora.cites\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists\n",
    "\n",
    "def run_cora():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    \n",
    "    num_nodes = 2708\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    \n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data),\n",
    "                                   requires_grad=False)\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    \n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True,\n",
    "                          cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(),\n",
    "                          cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(),\n",
    "                   enc1.embed_dim, 128, adj_lists, agg2, \n",
    "                   base_model=enc1, gcn=True, cuda=False)\n",
    "    \n",
    "    enc1.num_samples = 5\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    graphsage = SupervisedGraphSage(7, enc2)\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad,\n",
    "                                       graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    \n",
    "    for batch in range(100):\n",
    "        batch_nodes = train[:256]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        print (batch, loss.item())\n",
    "\n",
    "    val_output = graphsage.forward(val)\n",
    "    \n",
    "    print (\"Validation F1:\", f1_score(labels[val],\n",
    "                            val_output.data.numpy().argmax(axis=1),\n",
    "                            average=\"micro\"))\n",
    "    \n",
    "    print (\"Average batch time:\", np.mean(times))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_cora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n",
    "- [Graph Node Embedding Algorithms (Stanford - Fall 2019) by Jure Leskovec](https://www.youtube.com/watch?v=7JELX6DiUxQ)\n",
    "- [Jure Leskovec: \"Large-scale Graph Representation Learning\"](https://www.youtube.com/watch?v=oQL4E1gK3VU)\n",
    "- [Jure Leskovec \"Deep Learning on Graphs\"\n",
    "](https://www.youtube.com/watch?v=MIAbDNAxChI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
