{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 说明\n",
    "\n",
    "&emsp;&emsp;源自书籍《深入浅出图神经网络》第五章，使用GCN对Cora数据集进行节点分类，实战代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** 导入必要的库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import urllib\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** 保存处理好的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = namedtuple('Data', ['x','y','adjacency','train_mask','val_mask','test_mask'])\n",
    "\n",
    "class CoraData(object):\n",
    "    download_url = \"https://github.com/kimiyoung/planetoid/raw/master/data\"\n",
    "    filename = [\"ind.cora.{}\".format(name) for name in \n",
    "               ['x','tx','allx','y','ty','ally','graph','test.index']]\n",
    "    \n",
    "    def __init__(self, data_root = \"cora\", rebuild=False):\n",
    "        self.data_root = data_root\n",
    "        save_file = osp.join(self.data_root,\"processed_cora.pkl\")\n",
    "        \n",
    "        if osp.exists(save_file) and not rebuild:\n",
    "            print(\"Using Cached file: {}\".format(save_file))\n",
    "            self._data = pickle.load(open(save_file,\"rb\"))\n",
    "        else:\n",
    "            self.maybe_download()\n",
    "            self._data = self.process_data()\n",
    "            with open(save_file,\"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            \n",
    "            print(\"Cached file: {}\".format(save_file))\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "    \n",
    "    def maybe_download(self):\n",
    "        save_path = osp.join(self.data_root, \"raw\")\n",
    "        for name in self.filename:\n",
    "            if not osp.exists(osp.join(save_path, name)):\n",
    "                self.download_data(\"{}/{}\".format(self.download_url, name), save_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_data(url, save_path):\n",
    "        if not osp.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        data = urllib.request.urlopen(url)\n",
    "        filename = osp.basename(url)\n",
    "        \n",
    "        with open(osp.join(save_path, filename), \"wb\") as f:\n",
    "            f.write(data.read())\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def process_data(self):\n",
    "        print(\"Processing data...\")\n",
    "        _, tx, allx, y, ty, ally, graph, test_index = [self.read_data(\n",
    "        osp.join(self.data_root, \"raw\", name)) for name in self.filename]\n",
    "        \n",
    "        train_index = np.arange(y.shape[0])\n",
    "        val_index = np.arange(y.shape[0], y.shape[0] + 500)\n",
    "        sorted_test_index = sorted(test_index)\n",
    "        \n",
    "        x = np.concatenate((allx, tx), axis=0)\n",
    "        y = np.concatenate((ally, ty), axis=0).argmax(axis=1)\n",
    "        \n",
    "        x[test_index] = x[sorted_test_index]\n",
    "        y[test_index] = y[sorted_test_index]\n",
    "        \n",
    "        num_nodes = x.shape[0]\n",
    "        \n",
    "        train_mask = np.zeros(num_nodes, dtype=np.bool)\n",
    "        val_mask = np.zeros(num_nodes, dtype=np.bool)\n",
    "        test_mask = np.zeros(num_nodes, dtype=np.bool)\n",
    "        \n",
    "        train_mask[train_index] = True\n",
    "        val_mask[val_index] = True\n",
    "        test_mask[test_index] = True\n",
    "        \n",
    "        adjacency = self.build_adjacency(graph)\n",
    "        \n",
    "        print(\"Node's feature shape: \", x.shape)\n",
    "        print(\"Node's label shape: \", y.shape)\n",
    "        print(\"Adjacency's shape: \", adjacency.shape)\n",
    "        print(\"Number of training nodes: \", train_mask.sum())\n",
    "        \n",
    "        return Data(x=x, y=y, adjacency=adjacency, train_mask=train_mask, val_mask=val_mask\n",
    "                   ,test_mask=test_mask)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_adjacency(adj_dict):\n",
    "        edge_index = []\n",
    "        num_nodes = len(adj_dict)\n",
    "        \n",
    "        for src, dst in adj_dict.items():\n",
    "            edge_index.extend([src, v] for v in dst)\n",
    "            edge_index.extend([v, src] for v in dst)\n",
    "        \n",
    "        edge_index = list(k for k,_ in itertools.groupby(sorted(edge_index)))\n",
    "        edge_index = np.array(edge_index)\n",
    "        adjacency = sp.coo_matrix((np.ones(len(edge_index)),\n",
    "                                  (edge_index[:, 0], edge_index[:, 1])),\n",
    "                                 shape=(num_nodes, num_nodes), dtype=\"float32\")\n",
    "        \n",
    "        return adjacency\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data(path):\n",
    "        name = osp.basename(path)\n",
    "        if name==\"ind.cora.test.index\":\n",
    "            out = np.genfromtxt(path, dtype='int64')\n",
    "            return out\n",
    "        else:\n",
    "            out = pickle.load(open(path, \"rb\"), encoding=\"latin1\")\n",
    "            out = out.toarray() if hasattr(out, \"toarray\") else out\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Step 3:** 搭建GCN卷积层</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_normal_(self.weights)\n",
    "        if self.use_bias:\n",
    "            init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, adjacency, input_feature):\n",
    "        support = torch.mm(input_feature, self.weights)\n",
    "        output = torch.sparse.mm(adjacency, support)\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Step 4:** 搭建GCN整体模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_net(nn.Module):\n",
    "    def __init__(self, input_dim=1433):\n",
    "        super(GCN_net, self).__init__()\n",
    "        self.gcn1 = GraphConvolution(input_dim, 16)\n",
    "        self.gcn2 = GraphConvolution(16, 7)\n",
    "    \n",
    "    def forward(self, adjacency, feature):\n",
    "        h = F.relu(self.gcn1(adjacency, feature))\n",
    "        logits = self.gcn2(adjacency, h)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** 设置超参数与数据模型结合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cached file: cora\\processed_cora.pkl\n"
     ]
    }
   ],
   "source": [
    "def normalizetion(adjacency):\n",
    "    adjacency += sp.eye(adjacency.shape[0])\n",
    "    degree = np.array(adjacency.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    return d_hat.dot(adjacency).dot(d_hat).tocoo()\n",
    "\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "epochs = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN_net().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "dataset = CoraData().data\n",
    "x = dataset.x / dataset.x.sum(1, keepdims=True)\n",
    "\n",
    "tensor_x = torch.from_numpy(x).to(device)\n",
    "tensor_y = torch.from_numpy(dataset.y).to(device)\n",
    "\n",
    "tensor_train_mask = torch.from_numpy(dataset.train_mask).to(device)\n",
    "tensor_val_mask = torch.from_numpy(dataset.val_mask).to(device)\n",
    "tensor_test_mask = torch.from_numpy(dataset.test_mask).to(device)\n",
    "\n",
    "normalize_adjacency = normalizetion(dataset.adjacency)\n",
    "\n",
    "indices = torch.from_numpy(\n",
    "    np.asarray([normalize_adjacency.row,\n",
    "               normalize_adjacency.col]).astype('int64')).long()\n",
    "values = torch.from_numpy(normalize_adjacency.data.astype(np.float32))\n",
    "\n",
    "tensor_adjacency = torch.sparse.FloatTensor(indices, values, (2708, 2708)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss_history = []\n",
    "    val_acc_history = []\n",
    "    model.train()\n",
    "    train_y = tensor_y[tensor_train_mask]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        logits = model(tensor_adjacency, tensor_x)\n",
    "        train_mask_logits = logits[tensor_train_mask]\n",
    "        \n",
    "        loss = criterion(train_mask_logits, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc = test(tensor_train_mask)\n",
    "        val_acc = test(tensor_val_mask)\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        val_acc_history.append(val_acc.item())\n",
    "        \n",
    "        print(\"Epoch {:03d} : Loss {:.4f}, TrainACC {:.4}, ValACC {:.4f}\".format(\n",
    "                epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
    "        \n",
    "    return loss_history, val_acc_history\n",
    "\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_adjacency, tensor_x)\n",
    "        test_mask_logits = logits[mask]\n",
    "        predict_y = test_mask_logits.max(1)[1]\n",
    "        accuracy = torch.eq(predict_y, tensor_y[mask]).float().mean()\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 : Loss 1.9410, TrainACC 0.1786, ValACC 0.0920\n",
      "Epoch 001 : Loss 1.8645, TrainACC 0.5857, ValACC 0.4380\n",
      "Epoch 002 : Loss 1.7614, TrainACC 0.8429, ValACC 0.5960\n",
      "Epoch 003 : Loss 1.6250, TrainACC 0.8214, ValACC 0.5820\n",
      "Epoch 004 : Loss 1.4824, TrainACC 0.8929, ValACC 0.6620\n",
      "Epoch 005 : Loss 1.3102, TrainACC 0.9143, ValACC 0.7480\n",
      "Epoch 006 : Loss 1.1420, TrainACC 0.9286, ValACC 0.7360\n",
      "Epoch 007 : Loss 0.9759, TrainACC 0.9357, ValACC 0.7280\n",
      "Epoch 008 : Loss 0.8250, TrainACC 0.9357, ValACC 0.7480\n",
      "Epoch 009 : Loss 0.6890, TrainACC 0.9571, ValACC 0.7760\n",
      "Epoch 010 : Loss 0.5763, TrainACC 0.95, ValACC 0.7820\n",
      "Epoch 011 : Loss 0.4823, TrainACC 0.9786, ValACC 0.7760\n",
      "Epoch 012 : Loss 0.4087, TrainACC 0.9857, ValACC 0.7840\n",
      "Epoch 013 : Loss 0.3521, TrainACC 0.9857, ValACC 0.7920\n",
      "Epoch 014 : Loss 0.3076, TrainACC 0.9786, ValACC 0.7880\n",
      "Epoch 015 : Loss 0.2766, TrainACC 0.9786, ValACC 0.7900\n",
      "Epoch 016 : Loss 0.2517, TrainACC 0.9857, ValACC 0.7900\n",
      "Epoch 017 : Loss 0.2339, TrainACC 0.9857, ValACC 0.7820\n",
      "Epoch 018 : Loss 0.2224, TrainACC 0.9929, ValACC 0.7880\n",
      "Epoch 019 : Loss 0.2129, TrainACC 0.9929, ValACC 0.7920\n",
      "Epoch 020 : Loss 0.2077, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 021 : Loss 0.2023, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 022 : Loss 0.1987, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 023 : Loss 0.1938, TrainACC 1.0, ValACC 0.7880\n",
      "Epoch 024 : Loss 0.1900, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 025 : Loss 0.1843, TrainACC 1.0, ValACC 0.7880\n",
      "Epoch 026 : Loss 0.1794, TrainACC 1.0, ValACC 0.7880\n",
      "Epoch 027 : Loss 0.1733, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 028 : Loss 0.1680, TrainACC 1.0, ValACC 0.7860\n",
      "Epoch 029 : Loss 0.1620, TrainACC 1.0, ValACC 0.7860\n",
      "Epoch 030 : Loss 0.1573, TrainACC 1.0, ValACC 0.7860\n",
      "Epoch 031 : Loss 0.1527, TrainACC 1.0, ValACC 0.7840\n",
      "Epoch 032 : Loss 0.1492, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 033 : Loss 0.1458, TrainACC 1.0, ValACC 0.7880\n",
      "Epoch 034 : Loss 0.1430, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 035 : Loss 0.1408, TrainACC 1.0, ValACC 0.7840\n",
      "Epoch 036 : Loss 0.1386, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 037 : Loss 0.1369, TrainACC 1.0, ValACC 0.7840\n",
      "Epoch 038 : Loss 0.1350, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 039 : Loss 0.1333, TrainACC 1.0, ValACC 0.7840\n",
      "Epoch 040 : Loss 0.1318, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 041 : Loss 0.1303, TrainACC 1.0, ValACC 0.7760\n",
      "Epoch 042 : Loss 0.1299, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 043 : Loss 0.1296, TrainACC 1.0, ValACC 0.7760\n",
      "Epoch 044 : Loss 0.1305, TrainACC 1.0, ValACC 0.8120\n",
      "Epoch 045 : Loss 0.1264, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 046 : Loss 0.1221, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 047 : Loss 0.1216, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 048 : Loss 0.1229, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 049 : Loss 0.1226, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 050 : Loss 0.1209, TrainACC 1.0, ValACC 0.8100\n",
      "Epoch 051 : Loss 0.1224, TrainACC 1.0, ValACC 0.7880\n",
      "Epoch 052 : Loss 0.1238, TrainACC 1.0, ValACC 0.8100\n",
      "Epoch 053 : Loss 0.1195, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 054 : Loss 0.1176, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 055 : Loss 0.1196, TrainACC 1.0, ValACC 0.8080\n",
      "Epoch 056 : Loss 0.1186, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 057 : Loss 0.1177, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 058 : Loss 0.1180, TrainACC 1.0, ValACC 0.8080\n",
      "Epoch 059 : Loss 0.1175, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 060 : Loss 0.1173, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 061 : Loss 0.1177, TrainACC 1.0, ValACC 0.8060\n",
      "Epoch 062 : Loss 0.1168, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 063 : Loss 0.1163, TrainACC 1.0, ValACC 0.7860\n",
      "Epoch 064 : Loss 0.1170, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 065 : Loss 0.1166, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 066 : Loss 0.1156, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 067 : Loss 0.1160, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 068 : Loss 0.1163, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 069 : Loss 0.1158, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 070 : Loss 0.1154, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 071 : Loss 0.1158, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 072 : Loss 0.1160, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 073 : Loss 0.1156, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 074 : Loss 0.1157, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 075 : Loss 0.1159, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 076 : Loss 0.1156, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 077 : Loss 0.1155, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 078 : Loss 0.1158, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 079 : Loss 0.1156, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 080 : Loss 0.1155, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 081 : Loss 0.1155, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 082 : Loss 0.1157, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 083 : Loss 0.1158, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 084 : Loss 0.1157, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 085 : Loss 0.1157, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 086 : Loss 0.1158, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 087 : Loss 0.1158, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 088 : Loss 0.1158, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 089 : Loss 0.1158, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 090 : Loss 0.1159, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 091 : Loss 0.1160, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 092 : Loss 0.1160, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 093 : Loss 0.1159, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 094 : Loss 0.1159, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 095 : Loss 0.1160, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 096 : Loss 0.1161, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 097 : Loss 0.1161, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 098 : Loss 0.1161, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 099 : Loss 0.1161, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 100 : Loss 0.1162, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 101 : Loss 0.1162, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 102 : Loss 0.1163, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 103 : Loss 0.1164, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 104 : Loss 0.1166, TrainACC 1.0, ValACC 0.8040\n",
      "Epoch 105 : Loss 0.1169, TrainACC 1.0, ValACC 0.8040\n",
      "Epoch 106 : Loss 0.1179, TrainACC 1.0, ValACC 0.8080\n",
      "Epoch 107 : Loss 0.1192, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 108 : Loss 0.1219, TrainACC 1.0, ValACC 0.8060\n",
      "Epoch 109 : Loss 0.1227, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 110 : Loss 0.1200, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 111 : Loss 0.1129, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 112 : Loss 0.1132, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 113 : Loss 0.1181, TrainACC 1.0, ValACC 0.8040\n",
      "Epoch 114 : Loss 0.1168, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 115 : Loss 0.1147, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 116 : Loss 0.1176, TrainACC 1.0, ValACC 0.8080\n",
      "Epoch 117 : Loss 0.1200, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 118 : Loss 0.1187, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 119 : Loss 0.1159, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 120 : Loss 0.1167, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 121 : Loss 0.1179, TrainACC 1.0, ValACC 0.8060\n",
      "Epoch 122 : Loss 0.1161, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 123 : Loss 0.1154, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 124 : Loss 0.1169, TrainACC 1.0, ValACC 0.8040\n",
      "Epoch 125 : Loss 0.1169, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 126 : Loss 0.1163, TrainACC 1.0, ValACC 0.7860\n",
      "Epoch 127 : Loss 0.1165, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 128 : Loss 0.1170, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 129 : Loss 0.1167, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 130 : Loss 0.1160, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 131 : Loss 0.1164, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 132 : Loss 0.1167, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 133 : Loss 0.1161, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 134 : Loss 0.1160, TrainACC 1.0, ValACC 0.7880\n",
      "Epoch 135 : Loss 0.1166, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 136 : Loss 0.1165, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 137 : Loss 0.1160, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 138 : Loss 0.1160, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 139 : Loss 0.1164, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 140 : Loss 0.1163, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 141 : Loss 0.1159, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 142 : Loss 0.1159, TrainACC 1.0, ValACC 0.7860\n",
      "Epoch 143 : Loss 0.1163, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 144 : Loss 0.1161, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 145 : Loss 0.1160, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 146 : Loss 0.1160, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 147 : Loss 0.1160, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 148 : Loss 0.1160, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 149 : Loss 0.1158, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 150 : Loss 0.1158, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 151 : Loss 0.1160, TrainACC 1.0, ValACC 0.8040\n",
      "Epoch 152 : Loss 0.1159, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 153 : Loss 0.1158, TrainACC 1.0, ValACC 0.7940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154 : Loss 0.1157, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 155 : Loss 0.1158, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 156 : Loss 0.1158, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 157 : Loss 0.1158, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 158 : Loss 0.1157, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 159 : Loss 0.1157, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 160 : Loss 0.1157, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 161 : Loss 0.1157, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 162 : Loss 0.1156, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 163 : Loss 0.1156, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 164 : Loss 0.1157, TrainACC 1.0, ValACC 0.8020\n",
      "Epoch 165 : Loss 0.1157, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 166 : Loss 0.1158, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 167 : Loss 0.1156, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 168 : Loss 0.1156, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 169 : Loss 0.1155, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 170 : Loss 0.1156, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 171 : Loss 0.1157, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 172 : Loss 0.1158, TrainACC 1.0, ValACC 0.8000\n",
      "Epoch 173 : Loss 0.1159, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 174 : Loss 0.1160, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 175 : Loss 0.1160, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 176 : Loss 0.1164, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 177 : Loss 0.1164, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 178 : Loss 0.1171, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 179 : Loss 0.1170, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 180 : Loss 0.1174, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 181 : Loss 0.1162, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 182 : Loss 0.1152, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 183 : Loss 0.1141, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 184 : Loss 0.1145, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 185 : Loss 0.1160, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 186 : Loss 0.1172, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 187 : Loss 0.1182, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 188 : Loss 0.1176, TrainACC 1.0, ValACC 0.7900\n",
      "Epoch 189 : Loss 0.1168, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 190 : Loss 0.1148, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 191 : Loss 0.1140, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 192 : Loss 0.1145, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 193 : Loss 0.1158, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 194 : Loss 0.1171, TrainACC 1.0, ValACC 0.7980\n",
      "Epoch 195 : Loss 0.1169, TrainACC 1.0, ValACC 0.7920\n",
      "Epoch 196 : Loss 0.1164, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 197 : Loss 0.1153, TrainACC 1.0, ValACC 0.7940\n",
      "Epoch 198 : Loss 0.1149, TrainACC 1.0, ValACC 0.7960\n",
      "Epoch 199 : Loss 0.1151, TrainACC 1.0, ValACC 0.7960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.9409539699554443,\n",
       "  1.8645037412643433,\n",
       "  1.761406421661377,\n",
       "  1.625008463859558,\n",
       "  1.4823923110961914,\n",
       "  1.3102192878723145,\n",
       "  1.141982078552246,\n",
       "  0.9758604764938354,\n",
       "  0.8250491619110107,\n",
       "  0.6890226602554321,\n",
       "  0.5763481855392456,\n",
       "  0.4823465049266815,\n",
       "  0.4086972773075104,\n",
       "  0.352145791053772,\n",
       "  0.30763059854507446,\n",
       "  0.2766127288341522,\n",
       "  0.2517257034778595,\n",
       "  0.23393183946609497,\n",
       "  0.2223677784204483,\n",
       "  0.2128671407699585,\n",
       "  0.20768193900585175,\n",
       "  0.2022581696510315,\n",
       "  0.19869013130664825,\n",
       "  0.19375932216644287,\n",
       "  0.1899913251399994,\n",
       "  0.18426474928855896,\n",
       "  0.17939002811908722,\n",
       "  0.1732550859451294,\n",
       "  0.16800399124622345,\n",
       "  0.16197843849658966,\n",
       "  0.15734148025512695,\n",
       "  0.15267746150493622,\n",
       "  0.14915849268436432,\n",
       "  0.14576983451843262,\n",
       "  0.1430133879184723,\n",
       "  0.14083564281463623,\n",
       "  0.13859152793884277,\n",
       "  0.13690851628780365,\n",
       "  0.13495668768882751,\n",
       "  0.1332520693540573,\n",
       "  0.1317756623029709,\n",
       "  0.13026569783687592,\n",
       "  0.12986071407794952,\n",
       "  0.12955424189567566,\n",
       "  0.13053861260414124,\n",
       "  0.12643545866012573,\n",
       "  0.1221245750784874,\n",
       "  0.12159494310617447,\n",
       "  0.12288325279951096,\n",
       "  0.1225629523396492,\n",
       "  0.12088898569345474,\n",
       "  0.12240559607744217,\n",
       "  0.1238466426730156,\n",
       "  0.1194995641708374,\n",
       "  0.11760976165533066,\n",
       "  0.11956862360239029,\n",
       "  0.11855714023113251,\n",
       "  0.11765970289707184,\n",
       "  0.1179981529712677,\n",
       "  0.11745089292526245,\n",
       "  0.11727015674114227,\n",
       "  0.1176573634147644,\n",
       "  0.11682702600955963,\n",
       "  0.11629920452833176,\n",
       "  0.11697070300579071,\n",
       "  0.1166362464427948,\n",
       "  0.11562883108854294,\n",
       "  0.11598911136388779,\n",
       "  0.11628145724534988,\n",
       "  0.11577808111906052,\n",
       "  0.11542724817991257,\n",
       "  0.11582814902067184,\n",
       "  0.11598527431488037,\n",
       "  0.11560802161693573,\n",
       "  0.11565957963466644,\n",
       "  0.11592527478933334,\n",
       "  0.11561547964811325,\n",
       "  0.11552131921052933,\n",
       "  0.11575287580490112,\n",
       "  0.11564608663320541,\n",
       "  0.1154717206954956,\n",
       "  0.11545272916555405,\n",
       "  0.11569590121507645,\n",
       "  0.1158219501376152,\n",
       "  0.11565029621124268,\n",
       "  0.11566415429115295,\n",
       "  0.11577282100915909,\n",
       "  0.11578170210123062,\n",
       "  0.11579194664955139,\n",
       "  0.11580516397953033,\n",
       "  0.11591716855764389,\n",
       "  0.1159990057349205,\n",
       "  0.11600614339113235,\n",
       "  0.11591953784227371,\n",
       "  0.1159294918179512,\n",
       "  0.11598923802375793,\n",
       "  0.1161133348941803,\n",
       "  0.11611251533031464,\n",
       "  0.11611431837081909,\n",
       "  0.1160813719034195,\n",
       "  0.1161520853638649,\n",
       "  0.11618290096521378,\n",
       "  0.11631092429161072,\n",
       "  0.11635229736566544,\n",
       "  0.11659382283687592,\n",
       "  0.11686182022094727,\n",
       "  0.11785031855106354,\n",
       "  0.11917821317911148,\n",
       "  0.12188656628131866,\n",
       "  0.12269511818885803,\n",
       "  0.12004444748163223,\n",
       "  0.11291125416755676,\n",
       "  0.11316108703613281,\n",
       "  0.11805006116628647,\n",
       "  0.11681267619132996,\n",
       "  0.11474400758743286,\n",
       "  0.11762901395559311,\n",
       "  0.1200408935546875,\n",
       "  0.11870338022708893,\n",
       "  0.11586559563875198,\n",
       "  0.11665531247854233,\n",
       "  0.11788906157016754,\n",
       "  0.11606431752443314,\n",
       "  0.11543861031532288,\n",
       "  0.11689110845327377,\n",
       "  0.11693663150072098,\n",
       "  0.11629866063594818,\n",
       "  0.1165061816573143,\n",
       "  0.11697310209274292,\n",
       "  0.11667267978191376,\n",
       "  0.11603919416666031,\n",
       "  0.11644600331783295,\n",
       "  0.11670221388339996,\n",
       "  0.11605370044708252,\n",
       "  0.11600223183631897,\n",
       "  0.11661289632320404,\n",
       "  0.1164710596203804,\n",
       "  0.11601165682077408,\n",
       "  0.1159970685839653,\n",
       "  0.11635001003742218,\n",
       "  0.11633821576833725,\n",
       "  0.11592979729175568,\n",
       "  0.11594141274690628,\n",
       "  0.1162615492939949,\n",
       "  0.11612878739833832,\n",
       "  0.11595581471920013,\n",
       "  0.11600657552480698,\n",
       "  0.11600951105356216,\n",
       "  0.11601965874433517,\n",
       "  0.11582677066326141,\n",
       "  0.1158469095826149,\n",
       "  0.11601711064577103,\n",
       "  0.11593367904424667,\n",
       "  0.11584625393152237,\n",
       "  0.11574046313762665,\n",
       "  0.11577185243368149,\n",
       "  0.11584436893463135,\n",
       "  0.11577919870615005,\n",
       "  0.11574623733758926,\n",
       "  0.11571668833494186,\n",
       "  0.11568228900432587,\n",
       "  0.11569391191005707,\n",
       "  0.11564529687166214,\n",
       "  0.11562725901603699,\n",
       "  0.11570499837398529,\n",
       "  0.1157316192984581,\n",
       "  0.11576952040195465,\n",
       "  0.11561551690101624,\n",
       "  0.11557719111442566,\n",
       "  0.11550211161375046,\n",
       "  0.11556985229253769,\n",
       "  0.11565114557743073,\n",
       "  0.11580930650234222,\n",
       "  0.11585906147956848,\n",
       "  0.11599854379892349,\n",
       "  0.11599338799715042,\n",
       "  0.1163504347205162,\n",
       "  0.11643341928720474,\n",
       "  0.11710961163043976,\n",
       "  0.1170131117105484,\n",
       "  0.11737757921218872,\n",
       "  0.11616872996091843,\n",
       "  0.11517026275396347,\n",
       "  0.11413057893514633,\n",
       "  0.1144956722855568,\n",
       "  0.11595310270786285,\n",
       "  0.11716771870851517,\n",
       "  0.11821775138378143,\n",
       "  0.11763796210289001,\n",
       "  0.1167888268828392,\n",
       "  0.11482608318328857,\n",
       "  0.11395898461341858,\n",
       "  0.11449110507965088,\n",
       "  0.11579085886478424,\n",
       "  0.11706817895174026,\n",
       "  0.11691229790449142,\n",
       "  0.1163925752043724,\n",
       "  0.11528562754392624,\n",
       "  0.11485955119132996,\n",
       "  0.11509466171264648],\n",
       " [0.09200000017881393,\n",
       "  0.43799999356269836,\n",
       "  0.5960000157356262,\n",
       "  0.5820000171661377,\n",
       "  0.6620000004768372,\n",
       "  0.7480000257492065,\n",
       "  0.7360000014305115,\n",
       "  0.7279999852180481,\n",
       "  0.7480000257492065,\n",
       "  0.7760000228881836,\n",
       "  0.7820000052452087,\n",
       "  0.7760000228881836,\n",
       "  0.7839999794960022,\n",
       "  0.7919999957084656,\n",
       "  0.7879999876022339,\n",
       "  0.7900000214576721,\n",
       "  0.7900000214576721,\n",
       "  0.7820000052452087,\n",
       "  0.7879999876022339,\n",
       "  0.7919999957084656,\n",
       "  0.7940000295639038,\n",
       "  0.800000011920929,\n",
       "  0.7940000295639038,\n",
       "  0.7879999876022339,\n",
       "  0.7900000214576721,\n",
       "  0.7879999876022339,\n",
       "  0.7879999876022339,\n",
       "  0.7940000295639038,\n",
       "  0.7860000133514404,\n",
       "  0.7860000133514404,\n",
       "  0.7860000133514404,\n",
       "  0.7839999794960022,\n",
       "  0.7900000214576721,\n",
       "  0.7879999876022339,\n",
       "  0.7900000214576721,\n",
       "  0.7839999794960022,\n",
       "  0.7900000214576721,\n",
       "  0.7839999794960022,\n",
       "  0.7919999957084656,\n",
       "  0.7839999794960022,\n",
       "  0.7979999780654907,\n",
       "  0.7760000228881836,\n",
       "  0.800000011920929,\n",
       "  0.7760000228881836,\n",
       "  0.8119999766349792,\n",
       "  0.7940000295639038,\n",
       "  0.7900000214576721,\n",
       "  0.8019999861717224,\n",
       "  0.7900000214576721,\n",
       "  0.8019999861717224,\n",
       "  0.8100000023841858,\n",
       "  0.7879999876022339,\n",
       "  0.8100000023841858,\n",
       "  0.8019999861717224,\n",
       "  0.7960000038146973,\n",
       "  0.8080000281333923,\n",
       "  0.7979999780654907,\n",
       "  0.7940000295639038,\n",
       "  0.8080000281333923,\n",
       "  0.800000011920929,\n",
       "  0.7900000214576721,\n",
       "  0.8059999942779541,\n",
       "  0.7960000038146973,\n",
       "  0.7860000133514404,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.7960000038146973,\n",
       "  0.800000011920929,\n",
       "  0.7960000038146973,\n",
       "  0.7940000295639038,\n",
       "  0.800000011920929,\n",
       "  0.7960000038146973,\n",
       "  0.7940000295639038,\n",
       "  0.7960000038146973,\n",
       "  0.7940000295639038,\n",
       "  0.7979999780654907,\n",
       "  0.800000011920929,\n",
       "  0.7919999957084656,\n",
       "  0.8019999861717224,\n",
       "  0.7940000295639038,\n",
       "  0.7979999780654907,\n",
       "  0.800000011920929,\n",
       "  0.7900000214576721,\n",
       "  0.800000011920929,\n",
       "  0.7979999780654907,\n",
       "  0.7979999780654907,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973,\n",
       "  0.7979999780654907,\n",
       "  0.7960000038146973,\n",
       "  0.7979999780654907,\n",
       "  0.800000011920929,\n",
       "  0.8019999861717224,\n",
       "  0.800000011920929,\n",
       "  0.8019999861717224,\n",
       "  0.7979999780654907,\n",
       "  0.8019999861717224,\n",
       "  0.800000011920929,\n",
       "  0.7979999780654907,\n",
       "  0.7979999780654907,\n",
       "  0.8019999861717224,\n",
       "  0.7979999780654907,\n",
       "  0.8040000200271606,\n",
       "  0.8040000200271606,\n",
       "  0.8080000281333923,\n",
       "  0.7919999957084656,\n",
       "  0.8059999942779541,\n",
       "  0.7900000214576721,\n",
       "  0.8019999861717224,\n",
       "  0.800000011920929,\n",
       "  0.7940000295639038,\n",
       "  0.8040000200271606,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973,\n",
       "  0.8080000281333923,\n",
       "  0.800000011920929,\n",
       "  0.7979999780654907,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973,\n",
       "  0.8059999942779541,\n",
       "  0.7919999957084656,\n",
       "  0.7900000214576721,\n",
       "  0.8040000200271606,\n",
       "  0.7960000038146973,\n",
       "  0.7860000133514404,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973,\n",
       "  0.7919999957084656,\n",
       "  0.7900000214576721,\n",
       "  0.800000011920929,\n",
       "  0.7900000214576721,\n",
       "  0.7879999876022339,\n",
       "  0.7940000295639038,\n",
       "  0.7940000295639038,\n",
       "  0.7900000214576721,\n",
       "  0.7900000214576721,\n",
       "  0.7900000214576721,\n",
       "  0.800000011920929,\n",
       "  0.7960000038146973,\n",
       "  0.7860000133514404,\n",
       "  0.7979999780654907,\n",
       "  0.7900000214576721,\n",
       "  0.7979999780654907,\n",
       "  0.8019999861717224,\n",
       "  0.7919999957084656,\n",
       "  0.800000011920929,\n",
       "  0.7960000038146973,\n",
       "  0.7979999780654907,\n",
       "  0.8040000200271606,\n",
       "  0.7940000295639038,\n",
       "  0.7940000295639038,\n",
       "  0.7979999780654907,\n",
       "  0.7919999957084656,\n",
       "  0.800000011920929,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973,\n",
       "  0.7979999780654907,\n",
       "  0.7900000214576721,\n",
       "  0.8019999861717224,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973,\n",
       "  0.8019999861717224,\n",
       "  0.7979999780654907,\n",
       "  0.800000011920929,\n",
       "  0.7940000295639038,\n",
       "  0.7979999780654907,\n",
       "  0.7979999780654907,\n",
       "  0.7940000295639038,\n",
       "  0.7940000295639038,\n",
       "  0.800000011920929,\n",
       "  0.7940000295639038,\n",
       "  0.7979999780654907,\n",
       "  0.7940000295639038,\n",
       "  0.7979999780654907,\n",
       "  0.7900000214576721,\n",
       "  0.7979999780654907,\n",
       "  0.7960000038146973,\n",
       "  0.7979999780654907,\n",
       "  0.7900000214576721,\n",
       "  0.7979999780654907,\n",
       "  0.7960000038146973,\n",
       "  0.7919999957084656,\n",
       "  0.7960000038146973,\n",
       "  0.7919999957084656,\n",
       "  0.7979999780654907,\n",
       "  0.7900000214576721,\n",
       "  0.7960000038146973,\n",
       "  0.7940000295639038,\n",
       "  0.7960000038146973,\n",
       "  0.7979999780654907,\n",
       "  0.7919999957084656,\n",
       "  0.7979999780654907,\n",
       "  0.7919999957084656,\n",
       "  0.7960000038146973,\n",
       "  0.7940000295639038,\n",
       "  0.7960000038146973,\n",
       "  0.7960000038146973])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
